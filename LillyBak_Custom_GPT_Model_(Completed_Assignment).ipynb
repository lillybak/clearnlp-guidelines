{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lillybak/clearnlp-guidelines/blob/master/LillyBak_Custom_GPT_Model_(Completed_Assignment).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submission version 11/18/23 4:04 pm"
      ],
      "metadata": {
        "id": "OOcKAalTf9VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66mr2a2BtSpt",
        "outputId": "2a616e2a-5801-423b-c22d-6811cb52226a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 17 21:06:24 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Pre-Training of GPT-Style Model\n",
        "\n",
        "In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n",
        "\n",
        "The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n",
        "\n",
        "> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."
      ],
      "metadata": {
        "id": "UWiGVj6njoDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Selection\n",
        "\n",
        "For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n",
        "\n",
        "You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n",
        "\n",
        "> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n",
        "\n",
        "Let's start by grabbing our source repository for the day!"
      ],
      "metadata": {
        "id": "eHi04aEnkKEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRsEQZy6tgc",
        "outputId": "8a1ce903-9716-49ee-fd87-9c16cd26510d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 649, done.\u001b[K\n",
            "remote: Total 649 (delta 0), reused 0 (delta 0), pack-reused 649\u001b[K\n",
            "Receiving objects: 100% (649/649), 935.48 KiB | 5.96 MiB/s, done.\n",
            "Resolving deltas: 100% (373/373), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll need to grab some dependencies.\n",
        "\n",
        "`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."
      ],
      "metadata": {
        "id": "6l4CqoEDl7ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken requests cohere openai -qU"
      ],
      "metadata": {
        "id": "d_gepPv1Qdj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc4996e-91c9-4d2d-e91e-2f1c038db036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first - let's download our dataset!\n",
        "\n",
        "We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set. We want ~90% of our data to be training, and ~10% to be validation."
      ],
      "metadata": {
        "id": "70hSjXmZmCt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "current_path =\"/data/shakespeare/\" # last slash was missing\n",
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not os.path.exists(current_path):\n",
        "    os.makedirs(current_path)\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "print(n)\n",
        "\n"
      ],
      "metadata": {
        "id": "T7qRWArUNiZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0484c2-d6f8-45e3-cf24-27041dd06d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ../data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvmlzHSyW3Mj",
        "outputId": "8298788c-64c0-4c20-b905-7b9cf2c6f400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 2 root root 4096 Nov 17 21:06 shakespeare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ../data/shakespeare"
      ],
      "metadata": {
        "id": "xIDy4eFiyRGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013fc0f0-02f1-4e6a-9a3f-663d6dac6fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1092\n",
            "-rw-r--r-- 1 root root 1115394 Nov 17 21:06 input.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(current_path, input_file_path, os.path.dirname(current_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9lQmQU-Ua_h",
        "outputId": "4895e216-b64b-49ac-bd2d-80a6b9ef1eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/data/shakespeare/ /data/shakespeare/input.txt /data/shakespeare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directory /data/ now contains the raw text in file input.txt (from before I added the slash to \"shakespeare\"),   \n",
        " and the directory shakespeare with the same input.txt after I added it."
      ],
      "metadata": {
        "id": "CLWn3dhYRdzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_file_path, current_path, os.path.dirname(current_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0LyA7OOEOux",
        "outputId": "1d925b84-4bf6-4f47-f25c-862f81968933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/data/shakespeare/input.txt /data/shakespeare/ /data/shakespeare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqTvncwz_mbT",
        "outputId": "4503c1c4-839c-4541-9133-96f8aff56baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Befor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx1= int(.9*n)\n",
        "train_data = data[:idx1]\n",
        "val_data = data[idx1:]\n",
        "print(n,idx1)\n",
        "print(len(train_data), len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-475vUvH4KI",
        "outputId": "cb51e9da-da2b-47a7-fb99-ea95d561b462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394 1003854\n",
            "1003854 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."
      ],
      "metadata": {
        "id": "wU9BG2CymU-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers -qU"
      ],
      "metadata": {
        "id": "gFnrwKpQPsYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n",
        "\n",
        "Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmWXE5ctma9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is BPE?\n",
        "\n",
        "First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n",
        "\n",
        "The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n",
        "\n",
        "Let's take the following text and break it apart into its word components.\n",
        "\n",
        "\n",
        "```\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "```\n",
        "\n",
        "A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."
      ],
      "metadata": {
        "id": "GLecDiHbogvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "After pre-tokenization, a set of unique words has been created and the frequency\n",
        "with which each word occurred in the training data has been determined.\n",
        "Next, BPE creates a base vocabulary consisting of all symbols that occur in the\n",
        "set of unique words and learns merge rules to form a new symbol from two symbols\n",
        "of the base vocabulary. It does so until the vocabulary has attained the desired\n",
        "vocabulary size. Note that the desired vocabulary size is a hyperparameter to\n",
        "define before training the tokenizer.\n",
        "\"\"\"\n",
        "\n",
        "naive_word_list = input_text.split()"
      ],
      "metadata": {
        "id": "m34NDAGCpiz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can count our words and get their frequency."
      ],
      "metadata": {
        "id": "hR8k-2bopqjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "vocab_and_frequencies = defaultdict(int)\n",
        "\n",
        "for word in naive_word_list:\n",
        "  vocab_and_frequencies[\" \".join(list(word))] += 1\n",
        "\n",
        "sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)#[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_201bSQpvqD",
        "outputId": "953ed23e-f0c8-4452-e256-46c1ae0d8b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t h e', 8),\n",
              " ('a', 4),\n",
              " ('o f', 4),\n",
              " ('v o c a b u l a r y', 4),\n",
              " ('h a s', 3),\n",
              " ('s e t', 2),\n",
              " ('u n i q u e', 2),\n",
              " ('w o r d s', 2),\n",
              " ('b e e n', 2),\n",
              " ('a n d', 2),\n",
              " ('i n', 2),\n",
              " ('t r a i n i n g', 2),\n",
              " ('b a s e', 2),\n",
              " ('s y m b o l s', 2),\n",
              " ('t h a t', 2),\n",
              " ('t o', 2),\n",
              " ('d e s i r e d', 2),\n",
              " ('A f t e r', 1),\n",
              " ('p r e - t o k e n i z a t i o n ,', 1),\n",
              " ('c r e a t e d', 1),\n",
              " ('f r e q u e n c y', 1),\n",
              " ('w i t h', 1),\n",
              " ('w h i c h', 1),\n",
              " ('e a c h', 1),\n",
              " ('w o r d', 1),\n",
              " ('o c c u r r e d', 1),\n",
              " ('d a t a', 1),\n",
              " ('d e t e r m i n e d .', 1),\n",
              " ('N e x t ,', 1),\n",
              " ('B P E', 1),\n",
              " ('c r e a t e s', 1),\n",
              " ('c o n s i s t i n g', 1),\n",
              " ('a l l', 1),\n",
              " ('o c c u r', 1),\n",
              " ('l e a r n s', 1),\n",
              " ('m e r g e', 1),\n",
              " ('r u l e s', 1),\n",
              " ('f o r m', 1),\n",
              " ('n e w', 1),\n",
              " ('s y m b o l', 1),\n",
              " ('f r o m', 1),\n",
              " ('t w o', 1),\n",
              " ('v o c a b u l a r y .', 1),\n",
              " ('I t', 1),\n",
              " ('d o e s', 1),\n",
              " ('s o', 1),\n",
              " ('u n t i l', 1),\n",
              " ('a t t a i n e d', 1),\n",
              " ('s i z e .', 1),\n",
              " ('N o t e', 1),\n",
              " ('s i z e', 1),\n",
              " ('i s', 1),\n",
              " ('h y p e r p a r a m e t e r', 1),\n",
              " ('d e f i n e', 1),\n",
              " ('b e f o r e', 1),\n",
              " ('t o k e n i z e r .', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."
      ],
      "metadata": {
        "id": "NckufSxxp-w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List, Set\n",
        "\n",
        "def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n",
        "  vocab = set()\n",
        "\n",
        "  for word in current_vocab.keys():\n",
        "    for subword in word.split():\n",
        "      vocab.add(subword)\n",
        "\n",
        "  return len(vocab)"
      ],
      "metadata": {
        "id": "BNcjzjDvvKjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf3kCf-WvdBL",
        "outputId": "1e1c4ebf-6f71-4a89-90b3-21b02bc151e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are ~34 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."
      ],
      "metadata": {
        "id": "VoMq7GhKqf7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."
      ],
      "metadata": {
        "id": "OGxrHYmftDTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  pairs = {}\n",
        "\n",
        "  for word, frequency in current_vocab.items():\n",
        "    symbols = word.split()\n",
        "\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pair = (symbols[i], symbols[i + 1])\n",
        "      current_frequency = pairs.get(pair, 0)\n",
        "      pairs[pair] = current_frequency + frequency\n",
        "\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "sTwvfTAErQN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"
      ],
      "metadata": {
        "id": "FudOaKmYv9-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIJfkk7wFYw",
        "outputId": "b643a34a-3442-4a34-f044-4e25031ac4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('t', 'h'), 11),\n",
              " (('i', 'n'), 10),\n",
              " (('r', 'e'), 8),\n",
              " (('h', 'e'), 8),\n",
              " (('a', 't'), 7)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the frequent pairs - we can merge those pairs into a single token.\n",
        "\n",
        "Let's see how this process looks in code."
      ],
      "metadata": {
        "id": "OqORqdzwsZ6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  vocab_out = {}\n",
        "\n",
        "  pattern = re.escape(' '.join(most_common_pair))\n",
        "  replacement = ''.join(most_common_pair)\n",
        "\n",
        "  for word_in in current_vocab:\n",
        "      word_out = re.sub(pattern, replacement, word_in)\n",
        "      vocab_out[word_out] = current_vocab[word_in]\n",
        "\n",
        "  return vocab_out"
      ],
      "metadata": {
        "id": "L7ohHm2kshoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab_and_frequencies = merge_vocab(\n",
        "    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n",
        "    vocab_and_frequencies\n",
        ")"
      ],
      "metadata": {
        "id": "Ab760KKuwzZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0XtvLbpxbSx",
        "outputId": "843eefd9-4dfe-4d8c-9ef6-18a519729ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After one merge, we can see that `t h` has been converted to `th`!\n",
        "\n",
        "Let's see how that impacted our vocabulary."
      ],
      "metadata": {
        "id": "9DPkBzj2u-me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(new_vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO_xegCtxjQf",
        "outputId": "ca024855-80c7-4c3f-8a6e-e4073cf34e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n",
        "\n",
        "In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"
      ],
      "metadata": {
        "id": "o3M13D60xzZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Our Tokenizer\n",
        "\n",
        "Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n",
        "\n",
        "Let's walk through the steps we'll take:\n",
        "\n",
        "1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n",
        "\n",
        "  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n",
        "  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n",
        "\n",
        "2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n",
        "\n",
        "  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n",
        "\n",
        "3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n",
        "\n",
        "  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n",
        "  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"
      ],
      "metadata": {
        "id": "BePYCbHly02H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFD, Sequence\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.normalizer = Sequence([NFD()])\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "OrztE09OPosB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n",
        "\n",
        "Let's use the following:\n",
        "\n",
        "- `\"<s>\"`    : bos_token - beginning of sequence token\n",
        "- `\"</s>\"`   : eos_token - end of sequence token\n",
        "- `\"<pad>\"`  : padding_token - token used to pad sequences\n",
        "- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n",
        "- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n",
        "\n",
        "We're also going to set a target vocabulary of 50,000 tokens."
      ],
      "metadata": {
        "id": "dDqkNNdM1KsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(\n",
        "    vocab_size= 50000,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"<s>\",\"</s>\",\"<pad>\",\"<unk>\",\"<mask>\"]\n",
        ")"
      ],
      "metadata": {
        "id": "x9iQVhN3P3RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nothing left to do but point it at our data-source and let it train!\n",
        "\n",
        "We'll use the `.train()` method to accomplish this task.\n",
        "\n",
        "> NOTE: Pay attention to the desired inputs of the `.train()` method.\n",
        "\n",
        "- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"
      ],
      "metadata": {
        "id": "yQ8X9vZe2Fyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train([input_file_path], trainer)"
      ],
      "metadata": {
        "id": "LinLHotSP7gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"
      ],
      "metadata": {
        "id": "V2JNYiqB2qKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/tokenizer/'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.model.save(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk6QjDGHQy2K",
        "outputId": "d0f971c2-9f5b-44e0-9c86-3bfe70f45b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cMedB35asgi",
        "outputId": "a17511aa-3abf-4362-c780-54cba90350b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nanoGPT  sample_data  tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls tokenizer/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMZS-b88a7az",
        "outputId": "a4bf8cd7-fa6e-4953-ffa7-7910e73373b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merges.txt  vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -qU"
      ],
      "metadata": {
        "id": "cOOlbggdRFrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
      ],
      "metadata": {
        "id": "us1vofdhQ45C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it tokenizes our inputs!"
      ],
      "metadata": {
        "id": "0-Bnq7lV2xWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""
      ],
      "metadata": {
        "id": "dnYnFa3fTRLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = tokenizer(input_sentence)\n",
        "display(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PSHY5VufRbBj",
        "outputId": "266beb2f-b593-401b-cd85-51bd284edb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'input_ids': [12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_tokens = tokenizer.encode(tokenized_sentence[\"input_ids\"])\n",
        "encoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZrWzQQlTU41",
        "outputId": "9d67a988-d837-44fb-be85-accf33d043d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(encoded_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peBRh6qcWQFR",
        "outputId": "b006f955-a04b-412d-f441-8a78a41dd5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27241"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(encoded_tokens)#, skip_special_tokens = True)\n",
        "decoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oS6lE-NLRnzk",
        "outputId": "f8295b60-5f63-4839-be89-3e400f222e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing Dataset\n",
        "\n",
        "Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n",
        "\n",
        "We'll simply encode our training and validation data - and then save them in binary files for later!\n",
        "\n",
        "> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."
      ],
      "metadata": {
        "id": "ji3sF-rA21YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_data[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c8_OxXWGhKdl",
        "outputId": "6c30587d-8885-4175-da2a-16ffef53a599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = tokenizer.encode(tokenizer(train_data)[\"input_ids\"])\n",
        "val_ids =  tokenizer.encode(tokenizer(val_data)[\"input_ids\"])\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "calHML6JPnCU",
        "outputId": "76a7cdcd-5851-413d-bbc3-e85c2bdff6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 291,284 tokens\n",
            "val has 34,223 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(train_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTPJy0ZEWrj5",
        "outputId": "c90f5b21-942b-4f8e-eace-1f058c286eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "424544106"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train has 291,284 tokens\n",
        "val has 34,223 tokens"
      ],
      "metadata": {
        "id": "wlVpqqQrf8Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training The Model\n",
        "\n",
        "Now that we have our tokenized dataset, let's get to training our model!\n",
        "\n",
        "We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n",
        "\n",
        "First, let's literally jump into the `nanoGPT` repository we cloned earlier."
      ],
      "metadata": {
        "id": "c0I3VrRC3XIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# export to bin files\n",
        "data_path = \"/data/shakespeare/\"\n",
        "\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"
      ],
      "metadata": {
        "id": "nKJ1KqiiPkRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls /data/shakespeare/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN4yYt2nyV4Q",
        "outputId": "05133523-99f7-4a45-9a44-c4ea0d1acbf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.txt  train.bin  val.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUU2jaalUdqm",
        "outputId": "5ad8d316-7bdc-469b-a36a-49e5273a7f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlwNOfrIjBwS",
        "outputId": "ac1a3204-212b-4743-dafd-959026ffa7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34massets\u001b[0m/   \u001b[01;34mconfig\u001b[0m/          \u001b[01;34mdata\u001b[0m/    model.py   sample.py           train.py\n",
            "bench.py  configurator.py  LICENSE  README.md  scaling_laws.ipynb  transformer_sizing.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do some critical imports."
      ],
      "metadata": {
        "id": "13p1e8sa3k0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from the local repo\n",
        "from model import GPTConfig, GPT"
      ],
      "metadata": {
        "id": "weNR37BwUYNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-Parameters\n",
        "\n",
        "We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."
      ],
      "metadata": {
        "id": "kY_vWZG-3uM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I/O\n",
        "\n",
        "- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"
      ],
      "metadata": {
        "id": "OykCjVQK5EX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = 'out_checkpoints'"
      ],
      "metadata": {
        "id": "viM3qlWt5PVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization\n",
        "\n",
        "Since we're training from scratch, we'll use `init_from = 'scratch'`."
      ],
      "metadata": {
        "id": "A5iwwrNL5H4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_from = 'scratch'"
      ],
      "metadata": {
        "id": "OK1z2m3C312T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval and Logging\n",
        "\n",
        "- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n",
        "- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n",
        "- `eval_iters` - this is how *many* iterations we want to evaluate for.\n",
        "- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n",
        "- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."
      ],
      "metadata": {
        "id": "2YlolKOj4_dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 250\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "eval_only = False\n",
        "always_save_checkpoint = True"
      ],
      "metadata": {
        "id": "MbFN5Ltq4_mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "We can set our dataset here - we'll use the one we created earlier!"
      ],
      "metadata": {
        "id": "a488zaF_4zQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'shakespeare'"
      ],
      "metadata": {
        "id": "_QC7vWXC40Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Typical Hyper-Parameters\n",
        "\n",
        "- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n",
        "- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n",
        "- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."
      ],
      "metadata": {
        "id": "XP9rBgGc426Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 1\n",
        "batch_size = 16\n",
        "block_size = 512\n",
        "model_params_names = ['gradient_accumulation_steps','batch_size','block_size']\n",
        "model_params_values = [1,16,512]\n",
        "model_params = dict(zip(model_params_names,model_params_values))\n",
        "model_params"
      ],
      "metadata": {
        "id": "EM_ybLPP43Pd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8012f0-20a1-40b7-abd0-16e0de0a5c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gradient_accumulation_steps': 1, 'batch_size': 16, 'block_size': 512}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture\n",
        "\n",
        "- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n",
        "- `n_head` - this is the number of attention heads in each decoder layer!\n",
        "- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook. A default value of ~`500` should do the trick!\n",
        "- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n",
        "- `bias` - wether or not to use bias inside the LayerNorm/Linear layers."
      ],
      "metadata": {
        "id": "UZ-8bDIY45GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 504\n",
        "dropout = 0.2\n",
        "bias = True"
      ],
      "metadata": {
        "id": "gMyyDBxB6k4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_arch_names = ['n_layer', 'n_head', 'n_embd', 'dropout', 'bias']\n",
        "model_arch_values = [6,6,504,0.2,True]\n",
        "model_arch_args = dict(zip(model_arch_names,model_arch_values))"
      ],
      "metadata": {
        "id": "D5Rf2tWxb5cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ QUESTION:\n",
        "\n",
        "What condition must be true as it relates to the `n_embd` and `n_head`?"
      ],
      "metadata": {
        "id": "piNHkSaRNDjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ANSWER   \n",
        "The dimensionality of the embedding (n_embd) must be exactly divisible by n_head   \n",
        "Remainder (n_embd/n_head)=0 so that all of embedding dimensions can be handled by the heads.\n"
      ],
      "metadata": {
        "id": "JGe_jCJW8nFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer Hyper-Parameters\n",
        "\n",
        "Basic Optimizer Hyper-Parameters:\n",
        "\n",
        "- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n",
        "- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n",
        "\n",
        "Learning Rate Decay Settings:\n",
        "\n",
        "- `decay_lr` - set decay flag\n",
        "- `weight_Decay` - how much to decay lr by\n",
        "- `lr_decay_iters` - should be set to ~max_iters.\n",
        "- `min_lr` - the minimum lr, should be ~ lr / 10\n",
        "\n",
        "Clipping and Warmup:\n",
        "\n",
        "- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n",
        "- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n",
        "\n",
        "> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."
      ],
      "metadata": {
        "id": "3NWDTaAz7gwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adamw optimizer\n",
        "learning_rate = 0.001\n",
        "max_iters = 5000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "\n",
        "# lr decay settings\n",
        "decay_lr = True\n",
        "weight_decay = 0.1\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-4\n",
        "# clipping and warmup\n",
        "grad_clip = 1.0\n",
        "warmup_iters = 100"
      ],
      "metadata": {
        "id": "qe-669jwUptI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ QUESTION:\n",
        "\n",
        "Given a Learning Rate of `1e-4` and a maximum iteration cap of `10,000`: What should `lr_decay_iters` be, and what should `min_lr` be?"
      ],
      "metadata": {
        "id": "AzHvpMDTNfU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the Chinchilla paper, the optimal min learning rate $\\eta_{min}$ was found to be 10 times smaller than the initial learning rate:  \n",
        "   $\\eta_{min}=\\eta_0\\times10^{-1}=10^{-4}\\times10{-1} = 10^{-5}$   \n",
        "\n",
        "   The cos scheduler updates $\\eta$ as:   \n",
        "$\\eta_t = \\eta_{min} + \\frac{\\eta_0 - \\eta_{min}}{2} \\left(1+cos(\\frac{\\pi\\ t}{T})\\right)$  \n",
        "where $T$ is the iteration number \"lr_decay_iters\" when $\\eta_t$ = $\\eta_{min}$.  $\\left(1+cos(\\frac{\\pi\\ T}{T})\\right)=0 $  \n",
        "\n",
        "At that point we fix $\\eta_t = \\eta_{min}$.   \n",
        "Therefore $T<=10000$.\n"
      ],
      "metadata": {
        "id": "n4wx3cGM8-WL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."
      ],
      "metadata": {
        "id": "ucldc4mz9yeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend = 'nccl'\n",
        "device = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = True\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "master_process = True\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiGlMOp8Nux",
        "outputId": "072b0dc6-ddcb-4fb4-c440-073343336b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 8,192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Settings\n",
        "\n",
        "We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n",
        "\n",
        "Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."
      ],
      "metadata": {
        "id": "eKmdfbye-BNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "yh34QGD6VARU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n",
        "\n",
        "This block will:\n",
        "\n",
        "1. Set the data path\n",
        "2. Load the dataset we tokenized earlier from the `.bin` we saved\n",
        "3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."
      ],
      "metadata": {
        "id": "gKeNwYaZ-Zoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join('/data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "tOjaPyJpVEgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question:\n",
        "\n",
        "What can you tell us about the way the labels are generated?   \n",
        "In the code above     \n",
        "the \"input\" context window x contains a list of 512 tokens randomly selected from the data. The available indeces in the ***ix*** list are numbers in (0:len(data) - 512). We subtract the block_size 512, in order to include in x indeces in that range.   \n",
        "\n",
        "y is the desired output (\"label\"), which is a context window starting with x$_{i+1}$, the second token of the x list.   \n",
        "When the algorith sees x$_i$ it is desired to predict x$_{i+1}$ as its following token.\n",
        "\n",
        "So that the algorithm learns to predict the word that follows based on the previous word (or prior words and their positions).\n",
        "_____________________________________________________________\n",
        "\n",
        "Please produce an example of a single x and y pair.   \n",
        "\n",
        "x='Hello'   \n",
        "y=\"world\"   \n",
        "I tried some in https://transformer.huggingface.co/doc/gpt2-large\n",
        "This pair worked, but other predictions did not.\n"
      ],
      "metadata": {
        "id": "I-tifZVD-9hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Initialization of Model\n",
        "\n",
        "Here we init our number of iterations as 0, and our best val loss as a very high number."
      ],
      "metadata": {
        "id": "EbDlW-68_atH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9"
      ],
      "metadata": {
        "id": "6hsepdVBVzQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGrzSUn-YUlO",
        "outputId": "3b2eacfa-e563-461a-d9ae-807f4de388bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocabulary size: 20099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain our vocab size from our trained tokenizer."
      ],
      "metadata": {
        "id": "A4Uj9qBI_vXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = tokenizer.vocab_size\n",
        "meta_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m53DcCdFV0_a",
        "outputId": "55f75884-cb1f-419d-d730-5b31776570e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20099"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20099"
      ],
      "metadata": {
        "id": "W86S_3WxF2Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create our model args dict.\n",
        "\n",
        "Use the following as a guide: [Here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L109)"
      ],
      "metadata": {
        "id": "V7bcNelYARmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "504%6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lDWGgJBq4Ro",
        "outputId": "a7a4d171-6f63-4854-b8f3-42e8f8d75d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 504%6=0\n",
        "model_args = dict(n_layer = 6,\n",
        "n_head = 6,\n",
        "n_embd = 504,\n",
        "dropout = 0.01,\n",
        "bias = True)\n",
        "model_args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJSjUJKmpTZR",
        "outputId": "6c4a8d98-3b75-48b5-ffea-86d16624048b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_layer': 6, 'n_head': 6, 'n_embd': 504, 'dropout': 0.01, 'bias': True}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim_params = dict(learning_rate = 1e-3,\n",
        "max_iters = 5000,\n",
        "beta1 = 0.9,\n",
        "beta2 = 0.99,\n",
        "decay_lr = True,\n",
        "weight_decay = 0.1,\n",
        "lr_decay_iters = 5000,\n",
        "min_lr = 1e-4,\n",
        "grad_clip = 1.0,\n",
        "warmup_iters = 100\n",
        ")\n",
        "optim_params"
      ],
      "metadata": {
        "id": "JfIWEbanV7ZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b2a9e3-45c5-4b4e-c8c6-fd66d0c3cdb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.001,\n",
              " 'max_iters': 5000,\n",
              " 'beta1': 0.9,\n",
              " 'beta2': 0.99,\n",
              " 'decay_lr': True,\n",
              " 'weight_decay': 0.1,\n",
              " 'lr_decay_iters': 5000,\n",
              " 'min_lr': 0.0001,\n",
              " 'grad_clip': 1.0,\n",
              " 'warmup_iters': 100}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate our model with the provided `model_args`.\n",
        "\n",
        "These are derived from the hyper-parameters we set above."
      ],
      "metadata": {
        "id": "2WWcbkiCAUI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xly4iA0V-vF",
        "outputId": "24c852b8-d02d-414d-e7c2-6e281b6a1dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 28.46M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go! If you used the default values - you should have a model with 29.55M parameters!\n",
        "\n",
        "Let's set our block_size to the correct size as determined in our configuration steps."
      ],
      "metadata": {
        "id": "BpViOsxLAl6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size"
      ],
      "metadata": {
        "id": "TrEawNxdWRhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can look at our model in all its glory!"
      ],
      "metadata": {
        "id": "eRgguPLKAuZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "zaE3KSTnAtJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cecff77-4c32-4864-8394-d0ff9ab949bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(20099, 504)\n",
              "    (wpe): Embedding(1024, 504)\n",
              "    (drop): Dropout(p=0.01, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=504, out_features=1512, bias=True)\n",
              "          (c_proj): Linear(in_features=504, out_features=504, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.01, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.01, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=504, out_features=2016, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=2016, out_features=504, bias=True)\n",
              "          (dropout): Dropout(p=0.01, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=504, out_features=20099, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."
      ],
      "metadata": {
        "id": "LzoEY6gcBOSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "BNUThRt4WT5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."
      ],
      "metadata": {
        "id": "6Zs5Hcf9BBUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay,\n",
        "    learning_rate,\n",
        "    (beta1, beta2),\n",
        "    device_type\n",
        ")\n",
        "\n",
        "checkpoint = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesGeUnoWViL",
        "outputId": "a55fdd34-99ba-4221-fed1-d0b64cf287ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 28,677,096 parameters\n",
            "num non-decayed parameter tensors: 50, with 40,320 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compile our model!\n",
        "\n",
        "If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n",
        "\n",
        "Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
      ],
      "metadata": {
        "id": "ZF5YWJoKB4og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0"
      ],
      "metadata": {
        "id": "v0FNU0T0WXdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a1918c-f78e-446f-9468-24b3e536697c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n",
        "\n",
        "You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."
      ],
      "metadata": {
        "id": "p6lRcVsZCXRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "lUB5zVLVWbhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our LR Scheduler\n",
        "\n",
        "Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n",
        "\n",
        "We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n",
        "\n",
        "![img](https://i.imgur.com/KoFEl0b.png)\n",
        "\n",
        "There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"
      ],
      "metadata": {
        "id": "fLsOpaACDDkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / (1+warmup_iters)\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "7-mNpWBSWdHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "warmup_iters = 0\n",
        "lr_decay_iters = 10000\n",
        "min_lr = 10-5\n",
        "get_lr(10000)"
      ],
      "metadata": {
        "id": "Ni1fkZ7GmLD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set some specific values in our env to allow training in Colab."
      ],
      "metadata": {
        "id": "cqFePCZmE1Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nDL6s4YT6E",
        "outputId": "e0820c42-ec2a-4fa1-956a-2cdb88e41dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Loop\n",
        "\n",
        "Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"
      ],
      "metadata": {
        "id": "Nhqmxeo0Eg0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch('train')\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model\n",
        "running_mfu = -1.0 # model flops utilization\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ],
      "metadata": {
        "id": "kHbyEapRWmpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81e9c8c-cd55-45df-a9ac-c9db0074b39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 9.8996, val loss 9.8947\n",
            "iter 0: loss 9.8951, time 94223.69ms, mfu -100.00%\n",
            "iter 10: loss 8.4217, time 164.63ms, mfu 3.02%\n",
            "iter 20: loss 7.3160, time 168.09ms, mfu 3.01%\n",
            "iter 30: loss 6.2786, time 166.87ms, mfu 3.01%\n",
            "iter 40: loss 5.6951, time 165.77ms, mfu 3.01%\n",
            "iter 50: loss 5.4454, time 169.33ms, mfu 3.00%\n",
            "iter 60: loss 5.1830, time 169.41ms, mfu 2.99%\n",
            "iter 70: loss 4.8365, time 167.31ms, mfu 2.99%\n",
            "iter 80: loss 4.7903, time 171.01ms, mfu 2.98%\n",
            "iter 90: loss 4.7085, time 167.37ms, mfu 2.98%\n",
            "iter 100: loss 4.6781, time 169.00ms, mfu 2.98%\n",
            "iter 110: loss 4.5814, time 170.09ms, mfu 2.97%\n",
            "iter 120: loss 4.3981, time 170.44ms, mfu 2.97%\n",
            "iter 130: loss 4.2917, time 173.16ms, mfu 2.96%\n",
            "iter 140: loss 4.4361, time 173.43ms, mfu 2.95%\n",
            "iter 150: loss 4.1864, time 170.46ms, mfu 2.95%\n",
            "iter 160: loss 4.3221, time 174.45ms, mfu 2.94%\n",
            "iter 170: loss 4.0685, time 172.44ms, mfu 2.93%\n",
            "iter 180: loss 4.2650, time 171.57ms, mfu 2.93%\n",
            "iter 190: loss 4.1224, time 174.88ms, mfu 2.92%\n",
            "iter 200: loss 4.0388, time 176.93ms, mfu 2.91%\n",
            "iter 210: loss 4.0100, time 176.89ms, mfu 2.90%\n",
            "iter 220: loss 3.9222, time 176.06ms, mfu 2.89%\n",
            "iter 230: loss 3.8842, time 179.42ms, mfu 2.88%\n",
            "iter 240: loss 3.9696, time 174.94ms, mfu 2.87%\n",
            "step 250: train loss 3.8139, val loss 4.8935\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 250: loss 3.9065, time 20455.78ms, mfu 2.59%\n",
            "iter 260: loss 3.8900, time 186.29ms, mfu 2.60%\n",
            "iter 270: loss 3.7621, time 189.35ms, mfu 2.60%\n",
            "iter 280: loss 3.8083, time 190.82ms, mfu 2.60%\n",
            "iter 290: loss 3.5724, time 189.52ms, mfu 2.60%\n",
            "iter 300: loss 3.6311, time 189.43ms, mfu 2.61%\n",
            "iter 310: loss 3.7040, time 188.72ms, mfu 2.61%\n",
            "iter 320: loss 3.6269, time 193.08ms, mfu 2.60%\n",
            "iter 330: loss 3.6370, time 191.46ms, mfu 2.60%\n",
            "iter 340: loss 3.5143, time 191.80ms, mfu 2.60%\n",
            "iter 350: loss 3.6841, time 194.11ms, mfu 2.60%\n",
            "iter 360: loss 3.2922, time 195.09ms, mfu 2.59%\n",
            "iter 370: loss 3.4651, time 199.02ms, mfu 2.58%\n",
            "iter 380: loss 3.2900, time 191.13ms, mfu 2.59%\n",
            "iter 390: loss 3.5474, time 199.48ms, mfu 2.58%\n",
            "iter 400: loss 3.2030, time 198.52ms, mfu 2.57%\n",
            "iter 410: loss 3.2111, time 192.08ms, mfu 2.57%\n",
            "iter 420: loss 3.3653, time 195.50ms, mfu 2.57%\n",
            "iter 430: loss 3.1644, time 188.15ms, mfu 2.58%\n",
            "iter 440: loss 3.1524, time 189.87ms, mfu 2.58%\n",
            "iter 450: loss 2.9040, time 191.54ms, mfu 2.58%\n",
            "iter 460: loss 2.7886, time 195.19ms, mfu 2.58%\n",
            "iter 470: loss 2.8305, time 190.51ms, mfu 2.58%\n",
            "iter 480: loss 2.8721, time 188.69ms, mfu 2.59%\n",
            "iter 490: loss 2.7368, time 189.15ms, mfu 2.59%\n",
            "step 500: train loss 2.6748, val loss 5.2203\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 500: loss 2.7500, time 21541.37ms, mfu 2.33%\n",
            "iter 510: loss 2.7092, time 181.57ms, mfu 2.37%\n",
            "iter 520: loss 2.5734, time 183.85ms, mfu 2.41%\n",
            "iter 530: loss 2.3119, time 186.19ms, mfu 2.43%\n",
            "iter 540: loss 2.6267, time 183.08ms, mfu 2.46%\n",
            "iter 550: loss 2.2791, time 188.78ms, mfu 2.48%\n",
            "iter 560: loss 2.2101, time 188.72ms, mfu 2.49%\n",
            "iter 570: loss 2.3948, time 187.16ms, mfu 2.51%\n",
            "iter 580: loss 2.1455, time 189.74ms, mfu 2.52%\n",
            "iter 590: loss 2.2418, time 185.41ms, mfu 2.54%\n",
            "iter 600: loss 1.9817, time 189.43ms, mfu 2.55%\n",
            "iter 610: loss 1.9736, time 190.06ms, mfu 2.55%\n",
            "iter 620: loss 1.9648, time 188.21ms, mfu 2.56%\n",
            "iter 630: loss 1.6988, time 190.90ms, mfu 2.57%\n",
            "iter 640: loss 1.9905, time 193.82ms, mfu 2.57%\n",
            "iter 650: loss 1.8191, time 189.95ms, mfu 2.57%\n",
            "iter 660: loss 1.8713, time 189.10ms, mfu 2.58%\n",
            "iter 670: loss 1.6997, time 190.99ms, mfu 2.58%\n",
            "iter 680: loss 1.5219, time 191.04ms, mfu 2.58%\n",
            "iter 690: loss 1.4971, time 192.35ms, mfu 2.58%\n",
            "iter 700: loss 1.6841, time 191.84ms, mfu 2.58%\n",
            "iter 710: loss 1.2998, time 194.77ms, mfu 2.58%\n",
            "iter 720: loss 1.3619, time 187.34ms, mfu 2.59%\n",
            "iter 730: loss 1.4630, time 190.63ms, mfu 2.59%\n",
            "iter 740: loss 1.5826, time 193.92ms, mfu 2.59%\n",
            "step 750: train loss 1.2606, val loss 6.1053\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 750: loss 1.2352, time 21710.45ms, mfu 2.33%\n",
            "iter 760: loss 1.1426, time 184.07ms, mfu 2.37%\n",
            "iter 770: loss 1.1417, time 186.75ms, mfu 2.40%\n",
            "iter 780: loss 1.2211, time 187.10ms, mfu 2.42%\n",
            "iter 790: loss 1.1381, time 184.75ms, mfu 2.45%\n",
            "iter 800: loss 1.0385, time 185.57ms, mfu 2.47%\n",
            "iter 810: loss 1.1855, time 184.30ms, mfu 2.50%\n",
            "iter 820: loss 0.9950, time 184.09ms, mfu 2.52%\n",
            "iter 830: loss 0.9820, time 187.41ms, mfu 2.53%\n",
            "iter 840: loss 0.9270, time 188.05ms, mfu 2.54%\n",
            "iter 850: loss 1.0423, time 186.36ms, mfu 2.55%\n",
            "iter 860: loss 0.9338, time 187.14ms, mfu 2.56%\n",
            "iter 870: loss 0.9234, time 184.58ms, mfu 2.58%\n",
            "iter 880: loss 0.7736, time 184.36ms, mfu 2.59%\n",
            "iter 890: loss 0.9291, time 186.91ms, mfu 2.60%\n",
            "iter 900: loss 1.0042, time 184.24ms, mfu 2.61%\n",
            "iter 910: loss 0.8570, time 186.65ms, mfu 2.61%\n",
            "iter 920: loss 0.7294, time 189.41ms, mfu 2.61%\n",
            "iter 930: loss 0.6840, time 185.84ms, mfu 2.62%\n",
            "iter 940: loss 0.8232, time 188.65ms, mfu 2.62%\n",
            "iter 950: loss 0.6809, time 184.53ms, mfu 2.63%\n",
            "iter 960: loss 0.7332, time 189.18ms, mfu 2.63%\n",
            "iter 970: loss 0.6584, time 190.22ms, mfu 2.63%\n",
            "iter 980: loss 0.5959, time 189.87ms, mfu 2.63%\n",
            "iter 990: loss 0.5930, time 187.45ms, mfu 2.63%\n",
            "step 1000: train loss 0.5851, val loss 7.0852\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 1000: loss 0.6324, time 22302.81ms, mfu 2.37%\n",
            "iter 1010: loss 0.6635, time 182.68ms, mfu 2.40%\n",
            "iter 1020: loss 0.5980, time 188.11ms, mfu 2.43%\n",
            "iter 1030: loss 0.5996, time 188.65ms, mfu 2.45%\n",
            "iter 1040: loss 0.5726, time 185.99ms, mfu 2.47%\n",
            "iter 1050: loss 0.5749, time 187.56ms, mfu 2.49%\n",
            "iter 1060: loss 0.5440, time 185.80ms, mfu 2.51%\n",
            "iter 1070: loss 0.5256, time 184.35ms, mfu 2.53%\n",
            "iter 1080: loss 0.5200, time 187.58ms, mfu 2.54%\n",
            "iter 1090: loss 0.5303, time 191.05ms, mfu 2.54%\n",
            "iter 1100: loss 0.5422, time 184.71ms, mfu 2.56%\n",
            "iter 1110: loss 0.5429, time 188.68ms, mfu 2.57%\n",
            "iter 1120: loss 0.5365, time 184.89ms, mfu 2.58%\n",
            "iter 1130: loss 0.4445, time 188.31ms, mfu 2.59%\n",
            "iter 1140: loss 0.5142, time 187.17ms, mfu 2.59%\n",
            "iter 1150: loss 0.4793, time 186.21ms, mfu 2.60%\n",
            "iter 1160: loss 0.4229, time 188.26ms, mfu 2.60%\n",
            "iter 1170: loss 0.4267, time 186.89ms, mfu 2.61%\n",
            "iter 1180: loss 0.4149, time 188.48ms, mfu 2.61%\n",
            "iter 1190: loss 0.4258, time 187.18ms, mfu 2.62%\n",
            "iter 1200: loss 0.3933, time 189.61ms, mfu 2.62%\n",
            "iter 1210: loss 0.4214, time 185.35ms, mfu 2.62%\n",
            "iter 1220: loss 0.4168, time 186.23ms, mfu 2.63%\n",
            "iter 1230: loss 0.4318, time 187.84ms, mfu 2.63%\n",
            "iter 1240: loss 0.4460, time 185.02ms, mfu 2.64%\n",
            "step 1250: train loss 0.3712, val loss 7.6319\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 1250: loss 0.4769, time 21818.94ms, mfu 2.37%\n",
            "iter 1260: loss 0.4230, time 185.41ms, mfu 2.41%\n",
            "iter 1270: loss 0.3958, time 185.81ms, mfu 2.43%\n",
            "iter 1280: loss 0.3726, time 187.72ms, mfu 2.45%\n",
            "iter 1290: loss 0.3736, time 185.47ms, mfu 2.48%\n",
            "iter 1300: loss 0.3795, time 183.15ms, mfu 2.50%\n",
            "iter 1310: loss 0.3904, time 190.24ms, mfu 2.51%\n",
            "iter 1320: loss 0.3405, time 185.72ms, mfu 2.53%\n",
            "iter 1330: loss 0.3434, time 187.84ms, mfu 2.54%\n",
            "iter 1340: loss 0.3902, time 188.04ms, mfu 2.55%\n",
            "iter 1350: loss 0.3761, time 183.58ms, mfu 2.57%\n",
            "iter 1360: loss 0.3474, time 186.06ms, mfu 2.58%\n",
            "iter 1370: loss 0.3422, time 187.97ms, mfu 2.58%\n",
            "iter 1380: loss 0.3037, time 187.13ms, mfu 2.59%\n",
            "iter 1390: loss 0.2942, time 185.95ms, mfu 2.60%\n",
            "iter 1400: loss 0.3123, time 186.63ms, mfu 2.61%\n",
            "iter 1410: loss 0.3221, time 190.33ms, mfu 2.61%\n",
            "iter 1420: loss 0.3125, time 187.10ms, mfu 2.61%\n",
            "iter 1430: loss 0.2861, time 185.74ms, mfu 2.62%\n",
            "iter 1440: loss 0.2828, time 186.15ms, mfu 2.62%\n",
            "iter 1450: loss 0.2992, time 188.47ms, mfu 2.62%\n",
            "iter 1460: loss 0.3177, time 187.48ms, mfu 2.63%\n",
            "iter 1470: loss 0.3634, time 187.06ms, mfu 2.63%\n",
            "iter 1480: loss 0.3075, time 185.09ms, mfu 2.64%\n",
            "iter 1490: loss 0.2934, time 185.33ms, mfu 2.64%\n",
            "step 1500: train loss 0.2576, val loss 7.9903\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 1500: loss 0.2900, time 21677.70ms, mfu 2.38%\n",
            "iter 1510: loss 0.2728, time 183.31ms, mfu 2.41%\n",
            "iter 1520: loss 0.2805, time 187.96ms, mfu 2.44%\n",
            "iter 1530: loss 0.2790, time 186.08ms, mfu 2.46%\n",
            "iter 1540: loss 0.2675, time 185.85ms, mfu 2.48%\n",
            "iter 1550: loss 0.2682, time 188.34ms, mfu 2.50%\n",
            "iter 1560: loss 0.2751, time 187.07ms, mfu 2.51%\n",
            "iter 1570: loss 0.2596, time 185.39ms, mfu 2.53%\n",
            "iter 1580: loss 0.2600, time 187.99ms, mfu 2.54%\n",
            "iter 1590: loss 0.2359, time 186.33ms, mfu 2.55%\n",
            "iter 1600: loss 0.2976, time 185.94ms, mfu 2.57%\n",
            "iter 1610: loss 0.2318, time 186.75ms, mfu 2.58%\n",
            "iter 1620: loss 0.2848, time 185.95ms, mfu 2.59%\n",
            "iter 1630: loss 0.2486, time 187.99ms, mfu 2.59%\n",
            "iter 1640: loss 0.2530, time 187.04ms, mfu 2.60%\n",
            "iter 1650: loss 0.2515, time 188.23ms, mfu 2.60%\n",
            "iter 1660: loss 0.2155, time 188.25ms, mfu 2.61%\n",
            "iter 1670: loss 0.2070, time 187.20ms, mfu 2.61%\n",
            "iter 1680: loss 0.2010, time 188.34ms, mfu 2.61%\n",
            "iter 1690: loss 0.2201, time 184.18ms, mfu 2.62%\n",
            "iter 1700: loss 0.2036, time 185.95ms, mfu 2.63%\n",
            "iter 1710: loss 0.2389, time 185.77ms, mfu 2.63%\n",
            "iter 1720: loss 0.2153, time 186.34ms, mfu 2.64%\n",
            "iter 1730: loss 0.2337, time 181.83ms, mfu 2.65%\n",
            "iter 1740: loss 0.1791, time 185.49ms, mfu 2.65%\n",
            "step 1750: train loss 0.1872, val loss 8.3877\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 1750: loss 0.2421, time 21709.70ms, mfu 2.39%\n",
            "iter 1760: loss 0.2629, time 185.17ms, mfu 2.42%\n",
            "iter 1770: loss 0.2024, time 184.87ms, mfu 2.44%\n",
            "iter 1780: loss 0.2163, time 185.85ms, mfu 2.47%\n",
            "iter 1790: loss 0.2169, time 185.40ms, mfu 2.49%\n",
            "iter 1800: loss 0.2441, time 184.53ms, mfu 2.51%\n",
            "iter 1810: loss 0.2121, time 187.01ms, mfu 2.52%\n",
            "iter 1820: loss 0.1954, time 186.59ms, mfu 2.54%\n",
            "iter 1830: loss 0.2029, time 186.36ms, mfu 2.55%\n",
            "iter 1840: loss 0.2253, time 187.48ms, mfu 2.56%\n",
            "iter 1850: loss 0.1787, time 185.51ms, mfu 2.57%\n",
            "iter 1860: loss 0.2084, time 184.16ms, mfu 2.59%\n",
            "iter 1870: loss 0.2360, time 187.11ms, mfu 2.59%\n",
            "iter 1880: loss 0.1706, time 185.08ms, mfu 2.60%\n",
            "iter 1890: loss 0.1890, time 184.20ms, mfu 2.61%\n",
            "iter 1900: loss 0.1950, time 188.55ms, mfu 2.61%\n",
            "iter 1910: loss 0.1760, time 185.32ms, mfu 2.62%\n",
            "iter 1920: loss 0.1696, time 187.95ms, mfu 2.62%\n",
            "iter 1930: loss 0.1886, time 186.62ms, mfu 2.63%\n",
            "iter 1940: loss 0.2008, time 188.10ms, mfu 2.63%\n",
            "iter 1950: loss 0.2107, time 183.81ms, mfu 2.64%\n",
            "iter 1960: loss 0.1974, time 184.67ms, mfu 2.64%\n",
            "iter 1970: loss 0.1830, time 185.47ms, mfu 2.65%\n",
            "iter 1980: loss 0.1967, time 186.93ms, mfu 2.65%\n",
            "iter 1990: loss 0.1620, time 189.64ms, mfu 2.64%\n",
            "step 2000: train loss 0.1543, val loss 8.6162\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 2000: loss 0.1680, time 26566.59ms, mfu 2.38%\n",
            "iter 2010: loss 0.1622, time 179.83ms, mfu 2.42%\n",
            "iter 2020: loss 0.1710, time 180.96ms, mfu 2.45%\n",
            "iter 2030: loss 0.1926, time 182.22ms, mfu 2.48%\n",
            "iter 2040: loss 0.1580, time 183.41ms, mfu 2.50%\n",
            "iter 2050: loss 0.1641, time 182.93ms, mfu 2.52%\n",
            "iter 2060: loss 0.1781, time 182.25ms, mfu 2.55%\n",
            "iter 2070: loss 0.1745, time 188.30ms, mfu 2.55%\n",
            "iter 2080: loss 0.1802, time 183.16ms, mfu 2.57%\n",
            "iter 2090: loss 0.2124, time 185.97ms, mfu 2.58%\n",
            "iter 2100: loss 0.1844, time 188.88ms, mfu 2.59%\n",
            "iter 2110: loss 0.1769, time 190.42ms, mfu 2.59%\n",
            "iter 2120: loss 0.1459, time 188.54ms, mfu 2.59%\n",
            "iter 2130: loss 0.1456, time 189.32ms, mfu 2.60%\n",
            "iter 2140: loss 0.1427, time 191.01ms, mfu 2.60%\n",
            "iter 2150: loss 0.1727, time 192.47ms, mfu 2.60%\n",
            "iter 2160: loss 0.1639, time 190.75ms, mfu 2.60%\n",
            "iter 2170: loss 0.1603, time 186.83ms, mfu 2.60%\n",
            "iter 2180: loss 0.1654, time 187.48ms, mfu 2.61%\n",
            "iter 2190: loss 0.1378, time 188.77ms, mfu 2.61%\n",
            "iter 2200: loss 0.1785, time 185.93ms, mfu 2.62%\n",
            "iter 2210: loss 0.1459, time 188.44ms, mfu 2.62%\n",
            "iter 2220: loss 0.1586, time 184.53ms, mfu 2.63%\n",
            "iter 2230: loss 0.1562, time 186.98ms, mfu 2.63%\n",
            "iter 2240: loss 0.1373, time 187.24ms, mfu 2.63%\n",
            "step 2250: train loss 0.1275, val loss 8.8421\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 2250: loss 0.1434, time 25712.83ms, mfu 2.37%\n",
            "iter 2260: loss 0.1219, time 181.50ms, mfu 2.41%\n",
            "iter 2270: loss 0.1481, time 182.87ms, mfu 2.44%\n",
            "iter 2280: loss 0.1470, time 182.83ms, mfu 2.47%\n",
            "iter 2290: loss 0.1586, time 184.18ms, mfu 2.49%\n",
            "iter 2300: loss 0.1299, time 185.31ms, mfu 2.51%\n",
            "iter 2310: loss 0.1410, time 183.08ms, mfu 2.53%\n",
            "iter 2320: loss 0.1489, time 182.93ms, mfu 2.55%\n",
            "iter 2330: loss 0.1343, time 183.46ms, mfu 2.56%\n",
            "iter 2340: loss 0.1403, time 188.19ms, mfu 2.57%\n",
            "iter 2350: loss 0.1334, time 186.59ms, mfu 2.58%\n",
            "iter 2360: loss 0.1229, time 187.50ms, mfu 2.59%\n",
            "iter 2370: loss 0.1291, time 188.67ms, mfu 2.59%\n",
            "iter 2380: loss 0.1383, time 188.15ms, mfu 2.60%\n",
            "iter 2390: loss 0.1408, time 191.83ms, mfu 2.60%\n",
            "iter 2400: loss 0.1437, time 194.07ms, mfu 2.59%\n",
            "iter 2410: loss 0.1226, time 194.08ms, mfu 2.59%\n",
            "iter 2420: loss 0.1493, time 192.68ms, mfu 2.59%\n",
            "iter 2430: loss 0.1468, time 189.97ms, mfu 2.59%\n",
            "iter 2440: loss 0.1272, time 193.68ms, mfu 2.59%\n",
            "iter 2450: loss 0.1252, time 187.22ms, mfu 2.60%\n",
            "iter 2460: loss 0.1285, time 184.61ms, mfu 2.61%\n",
            "iter 2470: loss 0.1157, time 187.34ms, mfu 2.61%\n",
            "iter 2480: loss 0.1231, time 185.26ms, mfu 2.62%\n",
            "iter 2490: loss 0.1301, time 188.63ms, mfu 2.62%\n",
            "step 2500: train loss 0.1085, val loss 9.0111\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 2500: loss 0.1410, time 23070.94ms, mfu 2.36%\n",
            "iter 2510: loss 0.1095, time 181.62ms, mfu 2.40%\n",
            "iter 2520: loss 0.1282, time 183.10ms, mfu 2.43%\n",
            "iter 2530: loss 0.1191, time 181.45ms, mfu 2.46%\n",
            "iter 2540: loss 0.1252, time 183.71ms, mfu 2.48%\n",
            "iter 2550: loss 0.1225, time 184.53ms, mfu 2.51%\n",
            "iter 2560: loss 0.1068, time 181.40ms, mfu 2.53%\n",
            "iter 2570: loss 0.1175, time 181.51ms, mfu 2.55%\n",
            "iter 2580: loss 0.1269, time 181.75ms, mfu 2.57%\n",
            "iter 2590: loss 0.1199, time 182.36ms, mfu 2.58%\n",
            "iter 2600: loss 0.1095, time 186.58ms, mfu 2.59%\n",
            "iter 2610: loss 0.1164, time 187.73ms, mfu 2.60%\n",
            "iter 2620: loss 0.1368, time 186.40ms, mfu 2.60%\n",
            "iter 2630: loss 0.1070, time 186.27ms, mfu 2.61%\n",
            "iter 2640: loss 0.1074, time 183.73ms, mfu 2.62%\n",
            "iter 2650: loss 0.1262, time 188.77ms, mfu 2.62%\n",
            "iter 2660: loss 0.0966, time 182.97ms, mfu 2.63%\n",
            "iter 2670: loss 0.1178, time 186.18ms, mfu 2.64%\n",
            "iter 2680: loss 0.1192, time 186.75ms, mfu 2.64%\n",
            "iter 2690: loss 0.1111, time 185.51ms, mfu 2.64%\n",
            "iter 2700: loss 0.1050, time 187.35ms, mfu 2.64%\n",
            "iter 2710: loss 0.1066, time 186.83ms, mfu 2.65%\n",
            "iter 2720: loss 0.1118, time 187.43ms, mfu 2.65%\n",
            "iter 2730: loss 0.1059, time 183.28ms, mfu 2.65%\n",
            "iter 2740: loss 0.1095, time 190.18ms, mfu 2.65%\n",
            "step 2750: train loss 0.0922, val loss 8.9851\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 2750: loss 0.1012, time 26695.47ms, mfu 2.39%\n",
            "iter 2760: loss 0.1054, time 179.62ms, mfu 2.42%\n",
            "iter 2770: loss 0.1137, time 183.06ms, mfu 2.45%\n",
            "iter 2780: loss 0.1098, time 183.49ms, mfu 2.48%\n",
            "iter 2790: loss 0.1169, time 182.73ms, mfu 2.50%\n",
            "iter 2800: loss 0.1097, time 184.82ms, mfu 2.52%\n",
            "iter 2810: loss 0.1100, time 182.02ms, mfu 2.54%\n",
            "iter 2820: loss 0.1191, time 182.80ms, mfu 2.56%\n",
            "iter 2830: loss 0.1183, time 184.38ms, mfu 2.57%\n",
            "iter 2840: loss 0.1068, time 186.88ms, mfu 2.58%\n",
            "iter 2850: loss 0.1062, time 185.57ms, mfu 2.59%\n",
            "iter 2860: loss 0.0958, time 189.84ms, mfu 2.59%\n",
            "iter 2870: loss 0.1163, time 188.41ms, mfu 2.60%\n",
            "iter 2880: loss 0.1000, time 187.75ms, mfu 2.60%\n",
            "iter 2890: loss 0.0898, time 192.60ms, mfu 2.60%\n",
            "iter 2900: loss 0.0955, time 193.98ms, mfu 2.60%\n",
            "iter 2910: loss 0.0986, time 194.80ms, mfu 2.59%\n",
            "iter 2920: loss 0.1048, time 191.32ms, mfu 2.59%\n",
            "iter 2930: loss 0.0872, time 194.73ms, mfu 2.59%\n",
            "iter 2940: loss 0.0775, time 190.72ms, mfu 2.59%\n",
            "iter 2950: loss 0.1029, time 188.36ms, mfu 2.60%\n",
            "iter 2960: loss 0.0928, time 186.99ms, mfu 2.60%\n",
            "iter 2970: loss 0.0872, time 183.18ms, mfu 2.61%\n",
            "iter 2980: loss 0.1036, time 188.74ms, mfu 2.62%\n",
            "iter 2990: loss 0.0928, time 187.75ms, mfu 2.62%\n",
            "step 3000: train loss 0.0804, val loss 9.1233\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 3000: loss 0.0971, time 26520.20ms, mfu 2.36%\n",
            "iter 3010: loss 0.0981, time 178.81ms, mfu 2.40%\n",
            "iter 3020: loss 0.0911, time 178.29ms, mfu 2.44%\n",
            "iter 3030: loss 0.1001, time 182.66ms, mfu 2.47%\n",
            "iter 3040: loss 0.0789, time 183.15ms, mfu 2.49%\n",
            "iter 3050: loss 0.1094, time 183.11ms, mfu 2.51%\n",
            "iter 3060: loss 0.0905, time 183.82ms, mfu 2.53%\n",
            "iter 3070: loss 0.0853, time 182.79ms, mfu 2.55%\n",
            "iter 3080: loss 0.0796, time 180.76ms, mfu 2.57%\n",
            "iter 3090: loss 0.0810, time 182.16ms, mfu 2.59%\n",
            "iter 3100: loss 0.0954, time 187.71ms, mfu 2.59%\n",
            "iter 3110: loss 0.0892, time 189.70ms, mfu 2.60%\n",
            "iter 3120: loss 0.0727, time 190.25ms, mfu 2.60%\n",
            "iter 3130: loss 0.0769, time 188.46ms, mfu 2.60%\n",
            "iter 3140: loss 0.0850, time 190.74ms, mfu 2.60%\n",
            "iter 3150: loss 0.0883, time 190.61ms, mfu 2.60%\n",
            "iter 3160: loss 0.1068, time 191.04ms, mfu 2.60%\n",
            "iter 3170: loss 0.0759, time 190.67ms, mfu 2.60%\n",
            "iter 3180: loss 0.0925, time 196.43ms, mfu 2.60%\n",
            "iter 3190: loss 0.0709, time 187.65ms, mfu 2.60%\n",
            "iter 3200: loss 0.0739, time 186.68ms, mfu 2.61%\n",
            "iter 3210: loss 0.0708, time 189.18ms, mfu 2.61%\n",
            "iter 3220: loss 0.0934, time 188.24ms, mfu 2.61%\n",
            "iter 3230: loss 0.0822, time 185.65ms, mfu 2.62%\n",
            "iter 3240: loss 0.0869, time 188.24ms, mfu 2.62%\n",
            "step 3250: train loss 0.0718, val loss 9.2011\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 3250: loss 0.0754, time 26551.96ms, mfu 2.36%\n",
            "iter 3260: loss 0.0753, time 175.24ms, mfu 2.41%\n",
            "iter 3270: loss 0.0718, time 181.17ms, mfu 2.44%\n",
            "iter 3280: loss 0.0732, time 181.92ms, mfu 2.47%\n",
            "iter 3290: loss 0.0745, time 182.18ms, mfu 2.50%\n",
            "iter 3300: loss 0.0957, time 182.02ms, mfu 2.52%\n",
            "iter 3310: loss 0.0885, time 183.19ms, mfu 2.54%\n",
            "iter 3320: loss 0.0583, time 181.94ms, mfu 2.56%\n",
            "iter 3330: loss 0.0894, time 183.21ms, mfu 2.57%\n",
            "iter 3340: loss 0.0857, time 182.60ms, mfu 2.59%\n",
            "iter 3350: loss 0.0808, time 183.85ms, mfu 2.60%\n",
            "iter 3360: loss 0.0738, time 188.74ms, mfu 2.60%\n",
            "iter 3370: loss 0.0693, time 184.55ms, mfu 2.61%\n",
            "iter 3380: loss 0.0867, time 187.11ms, mfu 2.62%\n",
            "iter 3390: loss 0.0684, time 191.91ms, mfu 2.61%\n",
            "iter 3400: loss 0.0727, time 189.66ms, mfu 2.62%\n",
            "iter 3410: loss 0.0712, time 191.01ms, mfu 2.61%\n",
            "iter 3420: loss 0.0825, time 193.99ms, mfu 2.61%\n",
            "iter 3430: loss 0.0710, time 188.07ms, mfu 2.61%\n",
            "iter 3440: loss 0.0746, time 186.32ms, mfu 2.62%\n",
            "iter 3450: loss 0.0756, time 188.41ms, mfu 2.62%\n",
            "iter 3460: loss 0.0831, time 188.48ms, mfu 2.62%\n",
            "iter 3470: loss 0.0642, time 190.21ms, mfu 2.62%\n",
            "iter 3480: loss 0.0672, time 186.30ms, mfu 2.63%\n",
            "iter 3490: loss 0.0736, time 188.84ms, mfu 2.63%\n",
            "step 3500: train loss 0.0630, val loss 9.2228\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 3500: loss 0.0714, time 22390.97ms, mfu 2.37%\n",
            "iter 3510: loss 0.0602, time 184.61ms, mfu 2.40%\n",
            "iter 3520: loss 0.0730, time 182.08ms, mfu 2.43%\n",
            "iter 3530: loss 0.0673, time 180.07ms, mfu 2.46%\n",
            "iter 3540: loss 0.0802, time 180.26ms, mfu 2.49%\n",
            "iter 3550: loss 0.0758, time 181.79ms, mfu 2.52%\n",
            "iter 3560: loss 0.0673, time 179.96ms, mfu 2.54%\n",
            "iter 3570: loss 0.0651, time 182.05ms, mfu 2.56%\n",
            "iter 3580: loss 0.0624, time 180.66ms, mfu 2.58%\n",
            "iter 3590: loss 0.0706, time 182.48ms, mfu 2.59%\n",
            "iter 3600: loss 0.0605, time 181.73ms, mfu 2.61%\n",
            "iter 3610: loss 0.0587, time 181.82ms, mfu 2.62%\n",
            "iter 3620: loss 0.0745, time 181.11ms, mfu 2.63%\n",
            "iter 3630: loss 0.0635, time 182.20ms, mfu 2.64%\n",
            "iter 3640: loss 0.0592, time 182.03ms, mfu 2.65%\n",
            "iter 3650: loss 0.0612, time 185.54ms, mfu 2.65%\n",
            "iter 3660: loss 0.0708, time 184.32ms, mfu 2.66%\n",
            "iter 3670: loss 0.0552, time 185.10ms, mfu 2.66%\n",
            "iter 3680: loss 0.0545, time 185.69ms, mfu 2.66%\n",
            "iter 3690: loss 0.0635, time 186.23ms, mfu 2.66%\n",
            "iter 3700: loss 0.0575, time 187.95ms, mfu 2.66%\n",
            "iter 3710: loss 0.0502, time 186.60ms, mfu 2.66%\n",
            "iter 3720: loss 0.0601, time 187.24ms, mfu 2.66%\n",
            "iter 3730: loss 0.0695, time 187.61ms, mfu 2.66%\n",
            "iter 3740: loss 0.0712, time 189.11ms, mfu 2.66%\n",
            "step 3750: train loss 0.0575, val loss 9.3505\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 3750: loss 0.0674, time 22110.93ms, mfu 2.39%\n",
            "iter 3760: loss 0.0496, time 181.66ms, mfu 2.43%\n",
            "iter 3770: loss 0.0633, time 180.03ms, mfu 2.46%\n",
            "iter 3780: loss 0.0601, time 183.14ms, mfu 2.49%\n",
            "iter 3790: loss 0.0686, time 183.37ms, mfu 2.51%\n",
            "iter 3800: loss 0.0626, time 179.75ms, mfu 2.53%\n",
            "iter 3810: loss 0.0649, time 181.32ms, mfu 2.56%\n",
            "iter 3820: loss 0.0504, time 184.71ms, mfu 2.57%\n",
            "iter 3830: loss 0.0787, time 182.75ms, mfu 2.58%\n",
            "iter 3840: loss 0.0595, time 184.69ms, mfu 2.59%\n",
            "iter 3850: loss 0.0570, time 182.84ms, mfu 2.61%\n",
            "iter 3860: loss 0.0582, time 180.16ms, mfu 2.62%\n",
            "iter 3870: loss 0.0662, time 181.90ms, mfu 2.63%\n",
            "iter 3880: loss 0.0498, time 182.45ms, mfu 2.64%\n",
            "iter 3890: loss 0.0631, time 182.78ms, mfu 2.65%\n",
            "iter 3900: loss 0.0624, time 184.20ms, mfu 2.66%\n",
            "iter 3910: loss 0.0629, time 182.37ms, mfu 2.66%\n",
            "iter 3920: loss 0.0598, time 181.04ms, mfu 2.67%\n",
            "iter 3930: loss 0.0548, time 183.99ms, mfu 2.67%\n",
            "iter 3940: loss 0.0497, time 181.97ms, mfu 2.68%\n",
            "iter 3950: loss 0.0477, time 184.75ms, mfu 2.68%\n",
            "iter 3960: loss 0.0533, time 183.20ms, mfu 2.68%\n",
            "iter 3970: loss 0.0553, time 183.88ms, mfu 2.69%\n",
            "iter 3980: loss 0.0607, time 184.45ms, mfu 2.69%\n",
            "iter 3990: loss 0.0587, time 185.23ms, mfu 2.69%\n",
            "step 4000: train loss 0.0545, val loss 9.2601\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 4000: loss 0.0584, time 22267.06ms, mfu 2.42%\n",
            "iter 4010: loss 0.0542, time 178.71ms, mfu 2.46%\n",
            "iter 4020: loss 0.0694, time 184.86ms, mfu 2.48%\n",
            "iter 4030: loss 0.0468, time 186.53ms, mfu 2.50%\n",
            "iter 4040: loss 0.0533, time 179.79ms, mfu 2.52%\n",
            "iter 4050: loss 0.0552, time 184.74ms, mfu 2.54%\n",
            "iter 4060: loss 0.0502, time 184.69ms, mfu 2.56%\n",
            "iter 4070: loss 0.0604, time 185.00ms, mfu 2.57%\n",
            "iter 4080: loss 0.0635, time 187.31ms, mfu 2.58%\n",
            "iter 4090: loss 0.0537, time 182.40ms, mfu 2.59%\n",
            "iter 4100: loss 0.0615, time 185.17ms, mfu 2.60%\n",
            "iter 4110: loss 0.0663, time 184.09ms, mfu 2.61%\n",
            "iter 4120: loss 0.0466, time 184.86ms, mfu 2.62%\n",
            "iter 4130: loss 0.0638, time 184.14ms, mfu 2.63%\n",
            "iter 4140: loss 0.0556, time 184.35ms, mfu 2.63%\n",
            "iter 4150: loss 0.0487, time 183.32ms, mfu 2.64%\n",
            "iter 4160: loss 0.0699, time 183.48ms, mfu 2.65%\n",
            "iter 4170: loss 0.0593, time 185.14ms, mfu 2.65%\n",
            "iter 4180: loss 0.0531, time 184.38ms, mfu 2.66%\n",
            "iter 4190: loss 0.0592, time 182.18ms, mfu 2.66%\n",
            "iter 4200: loss 0.0353, time 183.25ms, mfu 2.67%\n",
            "iter 4210: loss 0.0558, time 184.18ms, mfu 2.67%\n",
            "iter 4220: loss 0.0478, time 182.71ms, mfu 2.68%\n",
            "iter 4230: loss 0.0697, time 185.16ms, mfu 2.68%\n",
            "iter 4240: loss 0.0498, time 181.34ms, mfu 2.68%\n",
            "step 4250: train loss 0.0497, val loss 9.2363\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 4250: loss 0.0516, time 21985.76ms, mfu 2.42%\n",
            "iter 4260: loss 0.0435, time 183.25ms, mfu 2.45%\n",
            "iter 4270: loss 0.0517, time 183.51ms, mfu 2.47%\n",
            "iter 4280: loss 0.0462, time 182.49ms, mfu 2.50%\n",
            "iter 4290: loss 0.0549, time 184.31ms, mfu 2.52%\n",
            "iter 4300: loss 0.0532, time 184.17ms, mfu 2.54%\n",
            "iter 4310: loss 0.0559, time 182.90ms, mfu 2.55%\n",
            "iter 4320: loss 0.0575, time 184.48ms, mfu 2.57%\n",
            "iter 4330: loss 0.0528, time 184.00ms, mfu 2.58%\n",
            "iter 4340: loss 0.0483, time 185.03ms, mfu 2.59%\n",
            "iter 4350: loss 0.0474, time 182.11ms, mfu 2.61%\n",
            "iter 4360: loss 0.0571, time 185.71ms, mfu 2.61%\n",
            "iter 4370: loss 0.0593, time 185.87ms, mfu 2.62%\n",
            "iter 4380: loss 0.0488, time 180.94ms, mfu 2.63%\n",
            "iter 4390: loss 0.0466, time 182.21ms, mfu 2.64%\n",
            "iter 4400: loss 0.0584, time 185.40ms, mfu 2.65%\n",
            "iter 4410: loss 0.0572, time 184.65ms, mfu 2.65%\n",
            "iter 4420: loss 0.0520, time 184.65ms, mfu 2.65%\n",
            "iter 4430: loss 0.0460, time 188.60ms, mfu 2.65%\n",
            "iter 4440: loss 0.0568, time 186.60ms, mfu 2.65%\n",
            "iter 4450: loss 0.0511, time 184.80ms, mfu 2.66%\n",
            "iter 4460: loss 0.0588, time 185.17ms, mfu 2.66%\n",
            "iter 4470: loss 0.0654, time 185.03ms, mfu 2.66%\n",
            "iter 4480: loss 0.0519, time 186.01ms, mfu 2.66%\n",
            "iter 4490: loss 0.0543, time 186.16ms, mfu 2.66%\n",
            "step 4500: train loss 0.0455, val loss 9.2383\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 4500: loss 0.0402, time 21866.10ms, mfu 2.40%\n",
            "iter 4510: loss 0.0696, time 181.07ms, mfu 2.43%\n",
            "iter 4520: loss 0.0427, time 183.97ms, mfu 2.46%\n",
            "iter 4530: loss 0.0524, time 184.88ms, mfu 2.48%\n",
            "iter 4540: loss 0.0514, time 185.12ms, mfu 2.50%\n",
            "iter 4550: loss 0.0545, time 185.00ms, mfu 2.52%\n",
            "iter 4560: loss 0.0390, time 185.83ms, mfu 2.54%\n",
            "iter 4570: loss 0.0531, time 185.70ms, mfu 2.55%\n",
            "iter 4580: loss 0.0420, time 185.93ms, mfu 2.56%\n",
            "iter 4590: loss 0.0460, time 183.44ms, mfu 2.58%\n",
            "iter 4600: loss 0.0580, time 181.04ms, mfu 2.60%\n",
            "iter 4610: loss 0.0518, time 181.91ms, mfu 2.61%\n",
            "iter 4620: loss 0.0515, time 182.90ms, mfu 2.62%\n",
            "iter 4630: loss 0.0454, time 182.94ms, mfu 2.63%\n",
            "iter 4640: loss 0.0510, time 182.08ms, mfu 2.64%\n",
            "iter 4650: loss 0.0484, time 181.04ms, mfu 2.65%\n",
            "iter 4660: loss 0.0423, time 182.81ms, mfu 2.66%\n",
            "iter 4670: loss 0.0447, time 182.32ms, mfu 2.66%\n",
            "iter 4680: loss 0.0564, time 186.02ms, mfu 2.67%\n",
            "iter 4690: loss 0.0520, time 181.54ms, mfu 2.67%\n",
            "iter 4700: loss 0.0392, time 182.31ms, mfu 2.68%\n",
            "iter 4710: loss 0.0510, time 182.57ms, mfu 2.68%\n",
            "iter 4720: loss 0.0481, time 185.23ms, mfu 2.68%\n",
            "iter 4730: loss 0.0422, time 184.09ms, mfu 2.68%\n",
            "iter 4740: loss 0.0372, time 182.16ms, mfu 2.69%\n",
            "step 4750: train loss 0.0439, val loss 9.2213\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 4750: loss 0.0626, time 22194.18ms, mfu 2.42%\n",
            "iter 4760: loss 0.0415, time 181.95ms, mfu 2.45%\n",
            "iter 4770: loss 0.0511, time 183.40ms, mfu 2.48%\n",
            "iter 4780: loss 0.0424, time 187.20ms, mfu 2.50%\n",
            "iter 4790: loss 0.0442, time 182.65ms, mfu 2.52%\n",
            "iter 4800: loss 0.0415, time 185.14ms, mfu 2.54%\n",
            "iter 4810: loss 0.0473, time 183.25ms, mfu 2.55%\n",
            "iter 4820: loss 0.0453, time 185.95ms, mfu 2.57%\n",
            "iter 4830: loss 0.0434, time 183.45ms, mfu 2.58%\n",
            "iter 4840: loss 0.0491, time 183.72ms, mfu 2.59%\n",
            "iter 4850: loss 0.0412, time 185.09ms, mfu 2.60%\n",
            "iter 4860: loss 0.0509, time 181.90ms, mfu 2.61%\n",
            "iter 4870: loss 0.0487, time 183.73ms, mfu 2.62%\n",
            "iter 4880: loss 0.0491, time 183.92ms, mfu 2.63%\n",
            "iter 4890: loss 0.0482, time 185.63ms, mfu 2.64%\n",
            "iter 4900: loss 0.0509, time 183.87ms, mfu 2.64%\n",
            "iter 4910: loss 0.0381, time 185.35ms, mfu 2.65%\n",
            "iter 4920: loss 0.0500, time 183.63ms, mfu 2.65%\n",
            "iter 4930: loss 0.0517, time 186.50ms, mfu 2.65%\n",
            "iter 4940: loss 0.0419, time 182.36ms, mfu 2.66%\n",
            "iter 4950: loss 0.0465, time 187.97ms, mfu 2.66%\n",
            "iter 4960: loss 0.0457, time 181.96ms, mfu 2.67%\n",
            "iter 4970: loss 0.0479, time 186.22ms, mfu 2.67%\n",
            "iter 4980: loss 0.0473, time 180.89ms, mfu 2.68%\n",
            "iter 4990: loss 0.0509, time 182.17ms, mfu 2.68%\n",
            "step 5000: train loss 0.0421, val loss 9.2530\n",
            "saving checkpoint to out_checkpoints\n",
            "iter 5000: loss 0.0501, time 22020.93ms, mfu 2.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Outputs with our New Model\n",
        "\n",
        "Now we can leverage the `sample.py` file to generate outputs from our model!"
      ],
      "metadata": {
        "id": "L2J5JlRxFJOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%ls ../*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UbZuP6Sf5og",
        "outputId": "b815edb9-07d1-4580-ebfc-5448f54fb734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../nanoGPT:\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/   configurator.py  model.py          README.md           train.py\n",
            "bench.py  \u001b[01;34mdata\u001b[0m/            \u001b[01;34mout_checkpoints\u001b[0m/  sample.py           transformer_sizing.ipynb\n",
            "\u001b[01;34mconfig\u001b[0m/   LICENSE          \u001b[01;34m__pycache__\u001b[0m/      scaling_laws.ipynb\n",
            "\n",
            "../sample_data:\n",
            "\u001b[01;32manscombe.json\u001b[0m*               california_housing_train.csv  mnist_train_small.csv\n",
            "california_housing_test.csv  mnist_test.csv                \u001b[01;32mREADME.md\u001b[0m*\n",
            "\n",
            "../tokenizer:\n",
            "merges.txt  vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -ll out_checkpoints/ckpt.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MLPXBPCepAC",
        "outputId": "75c07eb9-4848-463a-a849-a351a1d859d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 346926321 Nov 17 21:32 out_checkpoints/ckpt.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation Set Up and Model Loading"
      ],
      "metadata": {
        "id": "eo_QP1ITFfX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat"
      ],
      "metadata": {
        "id": "9UBh9HoEgbg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out_checkpoints' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "-vftqU9LheEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "FQRB3j7iiNkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2f1aa9-ad0e-4902-87cf-21edafb5fab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 28.46M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ],
      "metadata": {
        "id": "N1YAy8DriVZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tokenizer\n",
        "encode = lambda s: enc.encode(s)\n",
        "decode = lambda l: enc.decode(l)"
      ],
      "metadata": {
        "id": "KoB-5ZuLicAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation!"
      ],
      "metadata": {
        "id": "mkTQ9wo7FjYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "id": "NmTcaHCjii5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae22612-56ac-4abc-c455-567c520c575e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To the noble duke hath straitly given my mind;\n",
            "Ind that have been the man;\n",
            "Not so weak-hinged fancy, something savours\n",
            "Of tyranny and will ignoble make you,\n",
            "Yea, scandalous to the world.\n",
            "\n",
            "LEONTES:\n",
            "On your allegiance,\n",
            "Out of the chamber with her! Were I a tyrant,\n",
            "Where were her life? she durst not call me so,\n",
            "If she did know me one. Away with her!\n",
            "\n",
            "PAULINA:\n",
            "I pray you, do not push me; I'll be gone.\n",
            "Look to your babe, my lord; 'tis yours:\n",
            "Jove send her\n",
            "A better guiding spirit! What needs these hands?\n",
            "You, that are thus so tender o'er his follies,\n",
            "Will never do him good, not one of you.\n",
            "So, so: farewell; we are gone.\n",
            "\n",
            "LEONTES:\n",
            "Thou, traitor, hast set on thy wife to this.\n",
            "My child? away with't! Even thou, that hast\n",
            "A heart so tender o'er it, take it hence\n",
            "And see it instantly consumed with fire;\n",
            "Even thou and none but thou. Take it up straight:\n",
            "Within this hour bring me word 'tis done,\n",
            "And by good testimony, or I'll seize thy life,\n",
            "With what thou else call'st thine. If thou refuse\n",
            "And wilt encounter with my wrath, say so;\n",
            "The bastard brains with these my proper hands\n",
            "Shall I dash out. Go, take it to the fire;\n",
            "For thou set'st on thy wife.\n",
            "\n",
            "ANTIGONUS:\n",
            "I did not, sir:\n",
            "These lords, my noble fellows, if they please,\n",
            "Can clear me in't.\n",
            "\n",
            "Lords:\n",
            "We can: my royal liege,\n",
            "He is not guilty of her coming hither.\n",
            "\n",
            "LEONTES:\n",
            "You're liars all.\n",
            "\n",
            "First Lord:\n",
            "Beseech your highness, give us better credit:\n",
            "We have always truly served you, and beseech you\n",
            "So to esteem of us, and on our knees we beg,\n",
            "As recompense of our dear services\n",
            "Past and to come, that you do change this purpose,\n",
            "Which being so horrible, so bloody, must\n",
            "Lead on to some foul issue: we all kneel.\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "JULIET:\n",
            "Do not swear at all;\n",
            "Or, if thou wilt, swear by thy gracious self,\n",
            "Which is the god of my idolatry,\n",
            "And I'll believe thee.\n",
            "\n",
            "ROMEO:\n",
            "If my heart's dear love--\n",
            "\n",
            "JULIET:\n",
            "Well, do not swear: although I joy in thee,\n",
            "I have no joy of this contract to-night:\n",
            "It is too rash, too unadvised, too sudden;\n",
            "Too like the lightning, which doth cease to be\n",
            "Ere one can say 'It lightens.' Sweet, good night!\n",
            "This bud of love, by summer's ripening breath,\n",
            "May prove a beauteous flower when next we meet.\n",
            "Good night, good night! as sweet repose and rest\n",
            "Come to thy heart as that within my breast!\n",
            "\n",
            "ROMEO:\n",
            "O, wilt thou leave me so unsatisfied?\n",
            "\n",
            "JULIET:\n",
            "What satisfaction canst thou have to-night?\n",
            "\n",
            "ROMEO:\n",
            "The exchange of thy love's faithful vow for mine.\n",
            "\n",
            "JULIET:\n",
            "I gave thee mine before thou didst request it:\n",
            "And yet I would it were to give again.\n",
            "\n",
            "ROMEO:\n",
            "Wouldst thou withdraw it? for what purpose, love?\n",
            "\n",
            "JULIET:\n",
            "But to be frank, and give it thee again.\n",
            "And yet I wish but for the thing I have:\n",
            "My bounty is as boundless as the sea,\n",
            "My love as deep; the more I give to thee,\n",
            "The more I have, for both are infinite.\n",
            "I hear some noise within; dear love, adieu!\n",
            "Anon, good nurse! Sweet Montague, be true.\n",
            "Stay but a little, I will come again.\n",
            "\n",
            "ROMEO:\n",
            "O blessed, blessed night! I am afeard.\n",
            "Being in night, all this is but a dream,\n",
            "Too flattering-sweet to be substantial.\n",
            "\n",
            "JULIET:\n",
            "Three words, dear Romeo, and good night indeed.\n",
            "If that thy bent of love be honourable,\n",
            "Thy purpose marriage, send me word to-morrow,\n",
            "By one that I'll procure to come to thee,\n",
            "Where and what time thou wilt perform the rite;\n",
            "And all my fortunes at thy foot I'll lay\n",
            "And follow thee my lord\n",
            "---------------\n",
            "\n",
            "I think it, my brother lived: I,\n",
            "Yet thus; set you in the which I am:\n",
            "And thus darkly to none but yield to eyes?\n",
            "Prithee now, dispatch;\n",
            "He has, or none of man, as we are\n",
            "had not in charge he must be proclaimed betimes\n",
            "i' the morn; 'tis is,\n",
            "And that his state, as are the head.\n",
            "\n",
            "ESCALUS:\n",
            "I do desire you at your wisdom: speak not in our face.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The duke will be gone with you. Lay souls,\n",
            "And hang upon him.\n",
            "\n",
            "LUCIO:\n",
            "This may prove worse than hanging.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "\n",
            "ANGELO:\n",
            "O my dread lord,\n",
            "I should be guiltier than my guiltiness,\n",
            "To think I can be undiscernible,\n",
            "When I perceive your grace, like power divine,\n",
            "Hath look'd upon my passes. Then, good prince,\n",
            "No longer session hold upon my shame,\n",
            "But let my trial be mine own confession:\n",
            "Immediate sentence then and sequent death\n",
            "Is all the grace I beg.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Come hither, Mariana.\n",
            "Say, wast thou e'er contracted to this woman?\n",
            "\n",
            "ANGELO:\n",
            "I was, my lord.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Go take her hence, and marry her instantly.\n",
            "Do you the office, friar; which consummate,\n",
            "Return him here again. Go with him, provost.\n",
            "\n",
            "ESCALUS:\n",
            "My lord, I am more amazed at his dishonour\n",
            "Than at the strangeness of it.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Come hither, Isabel.\n",
            "Your friar is now your prince: as I was then\n",
            "Advertising and holy to your business,\n",
            "Not changing heart with habit, I am still\n",
            "Attorney'd at your service.\n",
            "\n",
            "ISABELLA:\n",
            "O, give me pardon,\n",
            "That I, your vassal, have employ'd and pain'd\n",
            "Your unknown sovereignty!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "You are pardon'd, Isabel:\n",
            "And now, dear maid, be you as free to us.\n",
            "Your brother's death, I know, sits at your heart;\n",
            "And you may marvel why I obscured myself,\n",
            "Labouring to save his life,\n",
            "---------------\n",
            "\n",
            "\n",
            "COMINIUS:\n",
            "I think 'twill serve, if he\n",
            "Can thereto frame his spirit.\n",
            "\n",
            "VOLUMNIA:\n",
            "He must, and will\n",
            "Prithee now, say you will, and go about it.\n",
            "\n",
            "CORIOLANUS:\n",
            "Must I go show them my unbarbed sconce?\n",
            "Must I with base tongue give my noble heart\n",
            "A lie that it must bear? Well, I will do't:\n",
            "Yet, were there but this single plot to lose,\n",
            "This mould of Marcius, they to dust should grind it\n",
            "And throw't against the wind. To the market-place!\n",
            "You have put me now to such a part which never\n",
            "I shall discharge to the life.\n",
            "\n",
            "COMINIUS:\n",
            "Come, come, we'll prompt you.\n",
            "\n",
            "VOLUMNIA:\n",
            "I prithee now, sweet son, as thou hast said\n",
            "My praises made thee first a soldier, so,\n",
            "To have my praise for this, perform a part\n",
            "Thou hast not done before.\n",
            "\n",
            "CORIOLANUS:\n",
            "Well, I must do't:\n",
            "Away, my disposition, and possess me\n",
            "Some harlot's spirit! my throat of war be turn'd,\n",
            "Which quired with my drum, into a pipe\n",
            "Small as an eunuch, or the virgin voice\n",
            "That babies lulls asleep! the smiles of knaves\n",
            "Tent in my cheeks, and schoolboys' tears take up\n",
            "The glasses of my sight! a beggar's tongue\n",
            "Make motion through my lips, and my arm'd knees,\n",
            "Who bow'd but in my stirrup, bend like his\n",
            "That hath received an alms! I will not do't,\n",
            "Lest I surcease to honour mine own truth\n",
            "And by my body's action teach my mind\n",
            "A most inherent baseness.\n",
            "\n",
            "VOLUMNIA:\n",
            "At thy choice, then:\n",
            "To beg of thee, it is my more dishonour\n",
            "Than thou of them. Come all to ruin; let\n",
            "Thy mother rather feel thy pride than fear\n",
            "Thy dangerous stoutness, for I mock at death\n",
            "With as big heart as thou. Do as thou list\n",
            "Thy valiantness was mine, thou suck'dst it from me,\n",
            "But owe thy pride thyself.\n",
            "\n",
            "CORIOLANUS:\n",
            "Pray, be content:\n",
            "Mother, I am going to the market-place;\n",
            "Chide\n",
            "---------------\n",
            "\n",
            "\n",
            "JULIET:\n",
            "Ay, if hither thou wilt be.\n",
            "\n",
            "ROMEO:\n",
            "\n",
            "JULIET:\n",
            "'Tis but thy name that calls\n",
            "And then is my ghostly father's faithful wife.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Romeo shall thank thee, good father, for us both.\n",
            "\n",
            "JULIET:\n",
            "And what news is his wife, and my enemy;\n",
            "Where did he that? by that kill'd my weddinglou have a cause to-day.\n",
            "\n",
            "ROMEO:\n",
            "I have forgot that, thou didst bower the ladies.\n",
            "Who's my nurse, and my case.\n",
            "Give me a desperate case.\n",
            "And Provost?\n",
            "\n",
            "Provost:\n",
            "Is it your father\n",
            "To accompany me to the suburbs.\n",
            "\n",
            "Provost:\n",
            "Is the name.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Air your will, sir, to your good comfort in\n",
            "Hath yet made the book of day.\n",
            "\n",
            "Provost:\n",
            "Besides, he must die to your party.\n",
            "I will save your business.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The contents of you, I'll write you to the duke:\n",
            "The duke's comfort, with hope of you\n",
            "Be you shall haveent him I must, or die to-morrow.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The contents of fight with like to the duke:\n",
            "shall anon over-read it at your pleasure; where you\n",
            "shall find, within these two days he will be here.\n",
            "This is a thing that Angelo knows not; for he this\n",
            "very day receives letters of strange tenor;\n",
            "perchance of the duke's death; perchance entering\n",
            "into some monastery; but, by chance, nothing of what\n",
            "is writ. Look, the unfolding star calls up the\n",
            "shepherd. Put not yourself into amazement how these\n",
            "things should be: all difficulties are but easy\n",
            "when they are known. Call your executioner, and off\n",
            "with Barnardine's head: I will give him a present\n",
            "shrift and advise him for a better place. Yet you\n",
            "are amazed; but this shall absolutely resolve you.\n",
            "Come away; it is almost clear dawn.\n",
            "\n",
            "POMPEY:\n",
            "I am as well acquainted here as I was in our house\n",
            "of profession: one would think it were Mistress\n",
            "Overdone's own house, for here\n",
            "---------------\n",
            "\n",
            "\n",
            "JULIET:\n",
            "Ay, if thou hadst fear'd their bones, and I thy news:\n",
            "Nay, come, thou art as dear saint, when thou hast need'st\n",
            "weet, sweet nurse, thy drift!\n",
            "\n",
            "LADY CAPULET:\n",
            "What more news than death is my fair?\n",
            "\n",
            "JULIET:\n",
            "How art thou that the villain madam?\n",
            "\n",
            "Nurse:\n",
            "Your father is, yet she is not here?\n",
            "\n",
            "Nurse:\n",
            "Your son, you are not well.\n",
            "\n",
            "JULIET:\n",
            "That is no pity's death, but a Montague is morrow;\n",
            "Or, then I know thou art.\n",
            "Thou art a Montague; if thou yet, be a Montague should be your queen,\n",
            "Nor I, holy how O, be not a fool!\n",
            "Some harlot strumpet upon, by the strumpet,\n",
            "Which on self-faced by their death\n",
            "Should all the dead saint which you do not take\n",
            "But that, by the saint which you get myself:\n",
            "Now, good my life,\n",
            "One kiss of my recreation: send it not like it.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "I cannot choose but by bitterly.\n",
            "\n",
            "LADY CAPULET:\n",
            "Then, my liege,--\n",
            "\n",
            "JULIET:\n",
            "What is your highness to your daughter Gloucester,\n",
            "To-morrow will take it to my mind.\n",
            "\n",
            "ROMEO:\n",
            "I will not at all;\n",
            "Nor wish'd to your wonder, but my gracious lord,\n",
            "Fell Warwick's allies;\n",
            "And make a perfect to fall.\n",
            "\n",
            "JULIET:\n",
            "Ancient damnation! O most wicked fiend!\n",
            "Is it more sin to wish me thus forsworn,\n",
            "Or to dispraise my lord with that same tongue\n",
            "Which she hath praised him with above compare\n",
            "So many thousand times? Go, counsellor;\n",
            "Thou and my bosom henceforth shall be twain.\n",
            "I'll to the friar, to know his remedy:\n",
            "If all else fail, myself have power to die.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "On Thursday, sir? the time is very short.\n",
            "\n",
            "PARIS:\n",
            "My father Capulet will have it so;\n",
            "And I am nothing slow to slack his haste.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "You say you do not know the lady's mind:\n",
            "Une\n",
            "---------------\n",
            "\n",
            "The young and her face of her face.\n",
            "\n",
            "CATESBY:\n",
            "My lord!\n",
            "\n",
            "KING RICHARD III:\n",
            "Send out a pursuivant where's power\n",
            "To wail the plain devil's inclination of mine.\n",
            "Accords not deny to my loving liege,\n",
            "And they can, my loving liege. I'll knocks.\n",
            "\n",
            "NORFOLK:\n",
            "We must both give and take, my gracious lord.\n",
            "\n",
            "KING RICHARD III:\n",
            "Up with my tent there! here will I lie tonight;\n",
            "But where to-morrow?  Well, all's one for that.\n",
            "Who hath descried the number of the foe?\n",
            "\n",
            "NORFOLK:\n",
            "Six or seven thousand is their utmost power.\n",
            "\n",
            "KING RICHARD III:\n",
            "Why, our battalion trebles that account:\n",
            "Besides, the king's name is a tower of strength,\n",
            "Which they upon the adverse party want.\n",
            "Up with my tent there! Valiant gentlemen,\n",
            "Let us survey the vantage of the field\n",
            "Call for some men of sound direction\n",
            "Let's want no discipline, make no delay,\n",
            "For, lords, to-morrow is a busy day.\n",
            "\n",
            "RICHMOND:\n",
            "The weary sun hath made a golden set,\n",
            "And by the bright track of his fiery car,\n",
            "Gives signal, of a goodly day to-morrow.\n",
            "Sir William Brandon, you shall bear my standard.\n",
            "Give me some ink and paper in my tent\n",
            "I'll draw the form and model of our battle,\n",
            "Limit each leader to his several charge,\n",
            "And part in just proportion our small strength.\n",
            "My Lord of Oxford, you, Sir William Brandon,\n",
            "And you, Sir Walter Herbert, stay with me.\n",
            "The Earl of Pembroke keeps his regiment:\n",
            "Good Captain Blunt, bear my good night to him\n",
            "And by the second hour in the morning\n",
            "Desire the earl to see me in my tent:\n",
            "Yet one thing more, good Blunt, before thou go'st,\n",
            "Where is Lord Stanley quarter'd, dost thou know?\n",
            "\n",
            "BLUNT:\n",
            "Unless I have mista'en his colours much,\n",
            "Which well I am assured I have not done,\n",
            "His regiment lies half a mile at least\n",
            "South from the mighty power of the king.\n",
            "\n",
            "RICHMOND\n",
            "---------------\n",
            "\n",
            "Thus could our eyes can fly no more joy,\n",
            "Ne'er came captive to my legs;\n",
            "The midwife's dismal clangour heard from far,\n",
            "'Warwick, revenge! brother, revenge my death!'\n",
            "So, underneath the belly of their steeds,\n",
            "That stain'd their fetlocks in his smoking blood,\n",
            "The noble gentleman gave up the ghost.\n",
            "\n",
            "WARWICK:\n",
            "Then let the earth be drunken with our blood:\n",
            "I'll kill my horse, because I will not fly.\n",
            "Why stand we like soft-hearted women here,\n",
            "Wailing our losses, whiles the foe doth rage;\n",
            "And look upon, as if the tragedy\n",
            "Were play'd in jest by counterfeiting actors?\n",
            "Here on my knee I vow to God above,\n",
            "I'll never pause again, never stand still,\n",
            "Till either death hath closed these eyes of mine\n",
            "Or fortune given me measure of revenge.\n",
            "\n",
            "EDWARD:\n",
            "O Warwick, I do bend my knee with thine;\n",
            "And in this vow do chain my soul to thine!\n",
            "And, ere my knee rise from the earth's cold face,\n",
            "I throw my hands, mine eyes, my heart to thee,\n",
            "Thou setter up and plucker down of kings,\n",
            "Beseeching thee, if with they will it stands\n",
            "That to my foes this body must be prey,\n",
            "Yet that thy brazen gates of heaven may ope,\n",
            "And give sweet passage to my sinful soul!\n",
            "Now, lords, take leave until we meet again,\n",
            "Where'er it be, in heaven or in earth.\n",
            "\n",
            "RICHARD:\n",
            "Brother, give me thy hand; and, gentle Warwick,\n",
            "Let me embrace thee in my weary arms:\n",
            "I, that did never weep, now melt with woe\n",
            "That winter should cut off our spring-time so.\n",
            "\n",
            "WARWICK:\n",
            "Away, away! Once more, sweet lords farewell.\n",
            "\n",
            "GEORGE:\n",
            "Yet let us all together to our troops,\n",
            "And give them leave to fly that will not stay;\n",
            "And call them pillars that will stand to us;\n",
            "And, if we thrive, promise them such rewards\n",
            "As victors wear at the Olympian games:\n",
            "This may plant courage in their quailing breasts;\n",
            "For yet is hope of life and victory.\n",
            "Forslow no longer, make we hence amain\n",
            "---------------\n",
            "\n",
            "LEONTES:\n",
            "Once more, take her hence.\n",
            "\n",
            "PAULINA:\n",
            "A most unworthy and unnatural lord\n",
            "Can do no more.\n",
            "\n",
            "LEONTES:\n",
            "I'll ha' thee burnt.\n",
            "\n",
            "PAULINA:\n",
            "I care not:\n",
            "It is an heretic that makes the fire,\n",
            "Not she which burns in't. I'll not call you tyrant;\n",
            "But this most cruel usage of your queen,\n",
            "Not able to produce more accusation\n",
            "Than your own weak-hinged fancy, something savours\n",
            "Of tyranny and will ignoble make you,\n",
            "Yea, scandalous to the world.\n",
            "\n",
            "LEONTES:\n",
            "On your allegiance,\n",
            "Out of the chamber with her! Were I a tyrant,\n",
            "Where were her life? she durst not call me so,\n",
            "If she did know me one. Away with her!\n",
            "\n",
            "PAULINA:\n",
            "I pray you, do not push me; I'll be gone.\n",
            "Look to your babe, my lord; 'tis yours:\n",
            "Jove send her\n",
            "A better guiding spirit! What needs these hands?\n",
            "You, that are thus so tender o'er his follies,\n",
            "Will never do him good, not one of you.\n",
            "So, so: farewell; we are gone.\n",
            "\n",
            "LEONTES:\n",
            "Thou, traitor, hast set on thy wife to this.\n",
            "My child? away with't! Even thou, that hast\n",
            "A heart so tender o'er it, take it hence\n",
            "And see it instantly consumed with fire;\n",
            "Even thou and none but thou. Take it up straight:\n",
            "Within this hour bring me word 'tis done,\n",
            "And by good testimony, or I'll seize thy life,\n",
            "With what thou else call'st thine. If thou refuse\n",
            "And wilt encounter with my wrath, say so;\n",
            "The bastard brains with these my proper hands\n",
            "Shall I dash out. Go, take it to the fire;\n",
            "For thou set'st on thy wife.\n",
            "\n",
            "ANTIGONUS:\n",
            "I did not, sir:\n",
            "These lords, my noble fellows, if they please,\n",
            "Can clear me in't.\n",
            "\n",
            "Lords:\n",
            "We can: my royal liege,\n",
            "He is not guilty of her coming hither.\n",
            "\n",
            "LEONTES:\n",
            "You're liars all.\n",
            "\n",
            "First Lord:\n",
            "Beseech your\n",
            "---------------\n",
            "\n",
            "Not to be't:\n",
            "I love the most patiently.\n",
            "\n",
            "First, I will go with you.\n",
            "\n",
            "CLARENCE:\n",
            "Didst thou not hear me, became of Antigonus,\n",
            "Some a world; even I think my very sad hours,\n",
            "And rail upon the devil: I came,\n",
            "This, his father's sake! would have in love;\n",
            "The youngest of his will be out, begone; the day.\n",
            "\n",
            "First Servant:\n",
            "Ay, so I; the warrant; theYou may be aursuivant:\n",
            "Good even, good Sir John, and wonder how,\n",
            "For Gloucester's dukedom, I hope to give a gift.\n",
            "\n",
            "GLOUCESTER:\n",
            "Why, my lord, this ring encompasseth finger.\n",
            "Even so thy breast encloseth my poor heart;\n",
            "Wear both of them, for both of them are thine.\n",
            "And if thy poor devoted suppliant may\n",
            "But beg one favour at thy gracious hand,\n",
            "Thou dost confirm his happiness for ever.\n",
            "\n",
            "LADY ANNE:\n",
            "What is it?\n",
            "\n",
            "GLOUCESTER:\n",
            "That it would please thee leave these sad designs\n",
            "To him that hath more cause to be a mourner,\n",
            "And presently repair to Crosby Place;\n",
            "Where, after I have solemnly interr'd\n",
            "At Chertsey monastery this noble king,\n",
            "And wet his grave with my repentant tears,\n",
            "I will with all expedient duty see you:\n",
            "For divers unknown reasons. I beseech you,\n",
            "Grant me this boon.\n",
            "\n",
            "LADY ANNE:\n",
            "With all my heart; and much it joys me too,\n",
            "To see you are become so penitent.\n",
            "Tressel and Berkeley, go along with me.\n",
            "\n",
            "GLOUCESTER:\n",
            "Bid me farewell.\n",
            "\n",
            "LADY ANNE:\n",
            "'Tis more than you deserve;\n",
            "But since you teach me how to flatter you,\n",
            "Imagine I have said farewell already.\n",
            "\n",
            "GLOUCESTER:\n",
            "Sirs, take up the corse.\n",
            "\n",
            "GENTLEMEN:\n",
            "Towards Chertsey, noble lord?\n",
            "\n",
            "GLOUCESTER:\n",
            "No, to White-Friars; there attend my coining.\n",
            "Was ever woman in this humour woo'd?\n",
            "Was ever woman in this humour won?\n",
            "I'll have her; but I will not keep her long.\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cgg91-fidXK",
        "outputId": "9e0c66a3-97eb-4263-fbde-212d04b26253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34massets\u001b[0m/   configurator.py  model.py          README.md           train.py\n",
            "bench.py  \u001b[01;34mdata\u001b[0m/            \u001b[01;34mout_checkpoints\u001b[0m/  sample.py           transformer_sizing.ipynb\n",
            "\u001b[01;34mconfig\u001b[0m/   LICENSE          \u001b[01;34m__pycache__\u001b[0m/      scaling_laws.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls out_checkpoints/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RAlpletjAal",
        "outputId": "dd30eec4-86c9-4348-bbec-429faac57206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ckpt.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ckpt.pt is 330.9 MB"
      ],
      "metadata": {
        "id": "6cr9VlWNkYRO"
      }
    }
  ]
}